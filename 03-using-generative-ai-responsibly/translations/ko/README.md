# 책임감 있게 생성형 AI 사용하기

[![책임감 있게 생성형 AI 사용하기](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)

> _위 이미지를 클릭하여 이 수업의 영상을 시청하세요_

AI, 특히 생성형 AI에 매료되기 쉽지만, 이를 책임감 있게 사용하는 방법을 고려해야 합니다. 출력이 공정하고 해롭지 않도록 보장하는 방법 등을 고려해야 합니다. 이 장에서는 언급된 맥락, 고려해야 할 사항, 그리고 AI 사용을 개선하기 위한 적극적인 단계를 제공하는 것을 목표로 합니다.

## 소개

이 수업에서 다룰 내용:

- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI를 우선시해야 하는 이유
- 책임감 있는 AI의 핵심 원칙과 이들이 생성형 AI와 어떻게 연관되는지
- 이러한 책임감 있는 AI 원칙을 전략과 도구를 통해 실천하는 방법

## 학습 목표

이 수업을 마치면 다음을 알게 될 것입니다:

- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI의 중요성
- 생성형 AI 애플리케이션을 구축할 때 책임감 있는 AI의 핵심 원칙을 언제 생각하고 적용해야 하는지
- 책임감 있는 AI 개념을 실천하기 위해 사용할 수 있는 도구와 전략

## 책임감 있는 AI 원칙

생성형 AI에 대한 흥분이 그 어느 때보다 높습니다. 이러한 흥분으로 많은 새로운 개발자, 관심, 그리고 자금이 이 분야로 유입되었습니다. 이는 생성형 AI를 사용하여 제품과 회사를 구축하려는 사람들에게 매우 긍정적이지만, 우리가 책임감 있게 진행하는 것도 중요합니다.

이 과정 전반에 걸쳐, 우리는 스타트업과 AI 교육 제품을 구축하는 데 초점을 맞추고 있습니다. 우리는 책임감 있는 AI의 원칙인 공정성, 포용성, 신뢰성/안전성, 보안 및 개인정보 보호, 투명성, 책임성을 사용할 것입니다. 이러한 원칙을 통해 우리는 이들이 우리 제품에서 생성형 AI 사용과 어떻게 관련되는지 탐구할 것입니다.

## 왜 책임감 있는 AI를 우선시해야 하는가

제품을 구축할 때, 사용자의 최선의 이익을 염두에 둔 인간 중심적 접근 방식을 취하면 최상의 결과를 얻을 수 있습니다.

생성형 AI의 독특한 점은 사용자에게 도움이 되는 답변, 정보, 지침, 콘텐츠를 생성하는 힘입니다. 이는 많은 수동 단계 없이 수행될 수 있어 매우 인상적인 결과를 낼 수 있습니다. 하지만 적절한 계획과 전략 없이는 불행히도 사용자, 제품, 그리고 사회 전체에 해로운 결과를 초래할 수도 있습니다.

이러한 잠재적으로 해로운 결과 중 일부(전부는 아님)를 살펴보겠습니다:

### 환각

환각은 LLM이 완전히 무의미하거나 다른 정보 출처를 기반으로 우리가 사실적으로 잘못된 것을 알고 있는 내용을 생성할 때를 설명하는 용어입니다.

예를 들어, 우리 스타트업에서 학생들이 모델에게 역사적 질문을 할 수 있는 기능을 만들었다고 가정해 봅시다. 한 학생이 `타이타닉의 유일한 생존자는 누구였나요?`라는 질문을 합니다.

모델은 다음과 같은 응답을 생성합니다:

![타이타닉의 유일한 생존자는 누구였나요라는 프롬프트](../../../03-using-generative-ai-responsibly/images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> _(출처: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))_

이는 매우 자신감 있고 철저한 답변입니다. 불행히도, 이는 잘못된 정보입니다. 최소한의 연구만으로도 타이타닉 재난의 생존자가 한 명 이상이었다는 것을 알 수 있습니다. 이 주제에 대해 연구를 막 시작한 학생에게, 이 답변은 의문을 제기하지 않고 사실로 받아들일 만큼 설득력이 있을 수 있습니다. 이로 인해 AI 시스템이 신뢰할 수 없게 되고 우리 스타트업의 평판에 부정적인 영향을 미칠 수 있습니다.

각 LLM의 반복마다 환각을 최소화하는 성능 개선을 볼 수 있었습니다. 이러한 개선에도 불구하고, 우리는 애플리케이션 개발자와 사용자로서 여전히 이러한 한계를 인식하고 있어야 합니다.

### 유해한 콘텐츠

앞 섹션에서 LLM이 잘못되거나 무의미한 응답을 생성하는 경우를 다뤘습니다. 우리가 인식해야 할 또 다른 위험은 모델이 유해한 콘텐츠로 응답하는 경우입니다.

유해한 콘텐츠는 다음과 같이 정의될 수 있습니다:

- 자해나 특정 그룹에 대한 해를 지시하거나 장려하는 내용
- 혐오적이거나 비하하는 내용
- 어떤 종류의 공격이나 폭력 행위를 계획하도록 안내하는 내용
- 불법 콘텐츠를 찾거나 불법 행위를 저지르는 방법을 제공하는 내용
- 성적으로 노골적인 내용을 표시하는 것

우리 스타트업에서는 이러한 유형의 콘텐츠가 학생들에게 보이지 않도록 적절한 도구와 전략을 갖추고 있어야 합니다.

### 공정성 부족

공정성은 "AI 시스템이 편견과 차별로부터 자유롭고 모든 사람을 공정하고 평등하게 대우하도록 보장하는 것"으로 정의됩니다. 생성형 AI 세계에서, 우리는 모델의 출력이 소외된 그룹에 대한 배타적 세계관을 강화하지 않도록 해야 합니다.

이러한 유형의 출력은 사용자에게 긍정적인 제품 경험을 구축하는 데 파괴적일 뿐만 아니라, 사회에 더 큰 해를 끼칩니다. 애플리케이션 개발자로서, 우리는 생성형 AI로 솔루션을 구축할 때 항상 광범위하고 다양한 사용자 기반을 염두에 두어야 합니다.

## 생성형 AI를 책임감 있게 사용하는 방법

이제 책임감 있는 생성형 AI의 중요성을 확인했으니, AI 솔루션을 책임감 있게 구축하기 위해 취할 수 있는 4가지 단계를 살펴보겠습니다:

![완화 주기](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### 잠재적 해를 측정하기

소프트웨어 테스팅에서, 우리는 애플리케이션에 대한 사용자의 예상 행동을 테스트합니다. 마찬가지로, 사용자가 가장 많이 사용할 것 같은 다양한 프롬프트 세트를 테스트하는 것은 잠재적 해를 측정하는 좋은 방법입니다.

우리 스타트업이 교육 제품을 구축하고 있으므로, 교육 관련 프롬프트 목록을 준비하는 것이 좋을 것입니다. 이는 특정 과목, 역사적 사실, 학생 생활에 대한 프롬프트를 포함할 수 있습니다.

### 잠재적 해를 완화하기

이제 모델과 그 응답으로 인한 잠재적 해를 예방하거나 제한할 수 있는 방법을 찾을 때입니다. 우리는 이를 4개의 다른 계층으로 볼 수 있습니다:

![완화 계층](../../images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **모델**. 올바른 사용 사례에 맞는 올바른 모델 선택하기. GPT-4와 같은 더 크고 복잡한 모델은 더 작고 구체적인 사용 사례에 적용될 때 유해한 콘텐츠의 위험을 더 많이 초래할 수 있습니다. 훈련 데이터를 사용하여 미세 조정하는 것도 유해한 콘텐츠의 위험을 줄입니다.

- **안전 시스템**. 안전 시스템은 해를 완화하는 데 도움이 되는 모델을 제공하는 플랫폼의 도구 및 구성 세트입니다. 이의 예로 Azure OpenAI 서비스의 콘텐츠 필터링 시스템이 있습니다. 시스템은 또한 탈옥 공격과 봇의 요청과 같은 원치 않는 활동을 감지해야 합니다.

- **메타프롬프트**. 메타프롬프트와 그라운딩은 특정 행동과 정보를 기반으로 모델을 지시하거나 제한하는 방법입니다. 이는 시스템 입력을 사용하여 모델의 특정 한계를 정의하는 것일 수 있습니다. 또한, 시스템의 범위나 도메인과 더 관련된 출력을 제공하는 것일 수 있습니다.

또한 검색 증강 생성(RAG)과 같은 기술을 사용하여 모델이 신뢰할 수 있는 소스의 선택에서만 정보를 가져오도록 할 수 있습니다. 이 과정의 후반부에 [검색 애플리케이션 구축](../../../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 수업이 있습니다.

- **사용자 경험**. 마지막 계층은 사용자가 우리 애플리케이션의 인터페이스를 통해 어떤 방식으로든 모델과 직접 상호작용하는 곳입니다. 이렇게 우리는 UI/UX를 설계하여 사용자가 모델에 보낼 수 있는 입력 유형을 제한하고 사용자에게 표시되는 텍스트나 이미지를 제한할 수 있습니다. AI 애플리케이션을 배포할 때, 우리는 또한 우리의 생성형 AI 애플리케이션이 할 수 있는 것과 할 수 없는 것에 대해 투명해야 합니다.

[AI 애플리케이션을 위한 UX 설계](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 전체 수업이 있습니다.

- **모델 평가**. LLM과 작업하는 것은 모델이 훈련된 데이터를 항상 제어할 수 있는 것은 아니기 때문에 어려울 수 있습니다. 그럼에도 불구하고, 우리는 항상 모델의 성능과 출력을 평가해야 합니다. 모델의 정확성, 유사성, 근거성, 출력의 관련성을 측정하는 것은 여전히 중요합니다. 이는 이해관계자와 사용자에게 투명성과 신뢰를 제공하는 데 도움이 됩니다.

### 책임감 있는 생성형 AI 솔루션 운영하기

AI 애플리케이션 주변에 운영 관행을 구축하는 것이 마지막 단계입니다. 여기에는 모든 규제 정책을 준수하고 있는지 확인하기 위해 법무 및 보안과 같은 스타트업의 다른 부분과 협력하는 것이 포함됩니다. 출시 전에, 우리는 또한 사용자에 대한 해가 커지는 것을 방지하기 위해 배포, 사고 처리, 롤백에 대한 계획을 세우고자 합니다.

## 도구

책임감 있는 AI 솔루션을 개발하는 작업이 많아 보일 수 있지만, 이는 노력할 가치가 있는 작업입니다. 생성형 AI 영역이 성장함에 따라, 개발자가 책임감을 워크플로우에 효율적으로 통합하는 데 도움이 되는 더 많은 도구가 성숙해질 것입니다. 예를 들어, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)는 API 요청을 통해 유해한 콘텐츠와 이미지를 감지하는 데 도움을 줄 수 있습니다.

## 복습

책임감 있는 AI 사용을 보장하기 위해 신경 써야 할 몇 가지 사항은 무엇인가요?

1. 답변이 정확한지.
2. 유해한 사용, AI가 범죄 목적으로 사용되지 않는지.
3. AI가 편견과 차별로부터 자유로운지 확인하는 것.

A: 2와 3이 정답입니다. 책임감 있는 AI는 유해한 영향과 편견을 완화하는 방법 등을 고려하는 데 도움을 줍니다.

## 🚀 도전 과제

[Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)에 대해 읽어보고 여러분의 사용에 적용할 수 있는 것이 무엇인지 확인해보세요.

## 잘 하셨습니다. 학습을 계속하세요

이 수업을 마친 후, [생성형 AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성형 AI 지식을 계속 향상시켜 보세요!

4장으로 넘어가 [프롬프트 엔지니어링 기초](../../../04-prompt-engineering-fundamentals/translations/ko/README.md?WT.mc_id=academic-105485-koreyst)에 대해 알아보겠습니다!
